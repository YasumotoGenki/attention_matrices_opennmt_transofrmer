# attention_matrices_opennmt_transofrmer
This code will provide additional debug function to output attention matrices from all heads in all layers.

**Be careful, this code is only for transformer translation with OpenNMT**

**You cannot use this for any other purpose (training, rnn model, and so on)**





## Usage

The usage is described the next 2 files.

[0_training.ipynb](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/0_training.ipynb),

[1_translation.ipynb](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/1_translation.ipynb)
