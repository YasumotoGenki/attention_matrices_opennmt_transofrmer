# attention_matrices_opennmt_transofrmer
This code will provide additional debug function to output attention matrices from all heads in all layers.

**Be careful, this code is only for transformer translation with OpenNMT**

**You cannot use this for any other purpose (training, rnn model, and so on)**

## Original code outputs
```
                    彼          は         水泳          が         得意          で          は          な         かっ          た          。 
        he  0.1926349  0.2135644 *0.3037547  0.1662329  0.0272146  0.0256509  0.0133634  0.0147541  0.0181808  0.0119007  0.0127486 
       was  0.0003296  0.0002715  0.0078232  0.0054593  0.2146050  0.0028785  0.0006307  0.0103063  0.0127522 *0.7397975  0.0051462 
       not *0.2972103  0.1988322  0.1497583  0.0059330  0.1654069  0.0012575  0.0006963  0.0025957  0.0013412  0.1657965  0.0111720 
      good  0.0726774  0.0635210  0.0339526  0.0030502 *0.2572910  0.0081822  0.0988948  0.1765248  0.1591378  0.0552028  0.0715652 
        at  0.0029933  0.0025115  0.0109872  0.0131064  0.0026235  0.0025539  0.1246577  0.1441116 *0.3401029  0.3384554  0.0178969 
  swimming  0.0001587  0.0002043 *0.9994454  0.0001555  0.0000120  0.0000039  0.0000073  0.0000008  0.0000015  0.0000027  0.0000078 
         .  0.0002052  0.0002118  0.0001796  0.0119230  0.0093502  0.0049550  0.1294626 *0.3055270  0.2840273  0.0903514  0.1638069 
      </s>  0.0774564  0.0717031  0.0039419  0.0056847  0.0809682  0.1477892  0.0580925  0.1051003  0.0485598 *0.3345482  0.0661556 
```

## Modified code outputs
```
layer: 1 || head: 1
                    彼          は         水泳          が         得意          で          は          な         かっ          た          。 
        he  0.1317690  0.1431092 *0.6492066  0.0621998  0.0006849  0.0008576  0.0014918  0.0022297  0.0019207  0.0056224  0.0009081 
       was  0.0011874  0.0013639  0.0133887  0.0139621  0.0087429  0.0320573 *0.3478778  0.2681966  0.1891317  0.0130612  0.1110303 
       not  0.0010208  0.0009970  0.0214519  0.0192029  0.0749326  0.0314771  0.0560722  0.0436250  0.0320744 *0.6824098  0.0367364 
      good  0.0002222  0.0001636  0.0007924  0.0088532 *0.4916196  0.2402480  0.0559826  0.1036002  0.0343166  0.0145008  0.0497008 
        at  0.0576358  0.0497589  0.0043088  0.0112185 *0.3325394  0.2438139  0.1574941  0.0710646  0.0165800  0.0355184  0.0200677 
  swimming  0.0075492  0.0065343  0.0198430  0.0475265  0.0167443  0.0219905  0.1683472  0.2541997 *0.2550970  0.0312095  0.1709588 
         . *0.5177081  0.4450698  0.0088729  0.0010436  0.0191833  0.0033670  0.0011628  0.0001845  0.0003695  0.0016854  0.0013532 
      </s> *0.4841074  0.3982586  0.0734532  0.0163170  0.0003891  0.0017990  0.0095470  0.0010076  0.0022769  0.0076561  0.0051879 
layer: 1 || head: 2
                    彼          は         水泳          が         得意          で          は          な         かっ          た          。 
        he  0.0292578  0.0316353  0.0005262  0.0002370  0.0010367  0.0019914  0.0302462  0.0094020  0.0091893 *0.8819256  0.0045523 
       was  0.0353563  0.0373492  0.0041485  0.0004376  0.0011323  0.0015732  0.0262167  0.0205722  0.0455045 *0.8202386  0.0074709 
       not  0.0095911  0.0107062  0.0223548  0.1388103  0.1576510 *0.4056246  0.0903982  0.0435534  0.0369983  0.0296234  0.0546886 
      good  0.0006642  0.0008153  0.0010357  0.0070738  0.2544952 *0.7129974  0.0016512  0.0033023  0.0021015  0.0017880  0.0140755 
        at  0.0007473  0.0007896  0.0020935  0.1212146 *0.5621742  0.1673461  0.0132722  0.0657197  0.0210916  0.0014378  0.0441134 
  swimming  0.0043402  0.0044556  0.0000607  0.0029863 *0.6194050  0.2679759  0.0145156  0.0260168  0.0119995  0.0013175  0.0469268 
         .  0.1603826  0.1392348 *0.4098374  0.1753191  0.0128001  0.0191576  0.0341322  0.0069451  0.0122139  0.0022830  0.0276941 
      </s> *0.2140017  0.1782235  0.0489036  0.0328684  0.0006416  0.0368659  0.1510854  0.0508401  0.1071648  0.0293944  0.1500104 
layer: 1 || head: 3
:
:
layer: 1 || head: 8
                    彼          は         水泳          が         得意          で          は          な         かっ          た          。 
        he  0.3478104 *0.4735487  0.0035925  0.0009586  0.0031230  0.0035417  0.0780348  0.0215698  0.0120877  0.0515696  0.0041633 
       was  0.0003286  0.0002743  0.0027970  0.0117333  0.0107491  0.0053473  0.0861003 *0.3763577  0.2177769  0.2354695  0.0530661 
       not  0.0005115  0.0003298  0.0038616  0.0709317  0.1085896  0.0326791  0.0693428  0.2934906 *0.3158128  0.0133608  0.0910898 
      good  0.0448307  0.0334580  0.0611839  0.1755371 *0.3090333  0.2544542  0.0088220  0.0115797  0.0265039  0.0511916  0.0234058 
        at  0.0008234  0.0006728  0.0147403  0.1317803 *0.4420794  0.1419555  0.0104053  0.0646397  0.0749026  0.0025299  0.1154708 
  swimming  0.0005797  0.0005336  0.0010351  0.0021427  0.0417500  0.0418149  0.0909618 *0.3247352  0.3203364  0.0636667  0.1124438 
         .  0.1796839  0.1689107  0.1079914 *0.1970929  0.0244714  0.0297553  0.0496972  0.0304264  0.0465375  0.1085676  0.0568659 
      </s>  0.2708955 *0.3104674  0.0454098  0.0302039  0.0381941  0.0125891  0.0342519  0.0280803  0.0515551  0.1516490  0.0267039 
layer: 1 || average of heads
                    彼          は         水泳          が         得意          で          は          な         かっ          た          。 
        he  0.1786996 *0.2257081  0.0920675  0.0374808  0.0106731  0.0304341  0.0591721  0.0978357  0.1020632  0.1304770  0.0353888 
       was  0.0105350  0.0107793  0.1092764  0.0192416  0.1728386  0.0410518  0.0654406  0.0962943  0.0606491 *0.3822903  0.0316029 
       not  0.0054034  0.0047831  0.0245330  0.0717077 *0.2210597  0.1726623  0.0567695  0.1620200  0.1267841  0.1021205  0.0521567 
      good  0.0504260  0.0453481  0.0318717  0.1177108 *0.2654963  0.1747552  0.0423088  0.1044711  0.1373985  0.0103830  0.0198304 
        at  0.0268135  0.0259066  0.0943185  0.0911292 *0.2650979  0.1301173  0.0476114  0.0807501  0.0816555  0.0750315  0.0815685 
  swimming  0.0892492  0.0990201  0.0105102  0.0205983 *0.1769026  0.1088134  0.1004055  0.1387219  0.1091694  0.0310975  0.1155120 
         . *0.2025152  0.1716521  0.0786495  0.0873945  0.0483648  0.0566515  0.0899894  0.0680842  0.1283659  0.0297058  0.0386271 
      </s> *0.2725388  0.2638777  0.0272360  0.0213143  0.0186445  0.0413082  0.0834257  0.0702214  0.0596614  0.0458264  0.0959455
:
:
```

## Requirements

I used [OpenNMT tool-kit (v 1.1.1)](https://github.com/OpenNMT/OpenNMT-py/tree/1.1.1)

Python 3.6.9 (in Google Colab emvironment)

Same as OpenNMT requirements (below file)

[requirements.opt.txt](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/requirements.opt.txt)

## Usage

The usage is described the next 2 files.

[0_training.ipynb](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/0_training.ipynb)

[1_translation.ipynb](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/1_translation.ipynb)

## Author

Genki Yasumoto @ Nara Institute of Science and Technology (NAIST)

Contact: yasumoto.genki.ye1{at}is.naist.jp

## LICENSE

This code is based on [MIT LICENSE](https://github.com/YasumotoGenki/attention_matrices_opennmt_transofrmer/blob/main/LICENSE.md)
